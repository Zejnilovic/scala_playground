---
layout: default
title: GH Test
---
<div class="readme">
<h1 id="enceladus">Enceladus</h1>

<h3 id="anamebuild_statusbuildstatus"><a name="build_status"/>Build Status</h3>

<p>| develop |
| ------------- |
| <a href="https://opensource.bigusdatus.com/jenkins/job/Absa-OSS-Projects/job/enceladus/job/develop/"><img src="https://opensource.bigusdatus.com/jenkins/buildStatus/icon?job=Absa-OSS-Projects%2Fenceladus%2Fdevelop" alt="Build Status" /></a>  | </p>

<hr />

<!-- toc -->
- [What is Enceladus?](#what-is-enceladus)
    - [Menas](#wie-menas)
    - [Standardization](#wie-standardization)
    - [Conformance](#wie-conformance)
- [How to build](#build)
- [How to run](#run)
- [How to contribute](#contribute)
- [How to use](#use)
    - [Standardization](#use-standardization)
<!-- tocstop -->

<h2 id="anamewhatisenceladuswhatisenceladus"><a name="what-is-enceladus"/>What is Enceladus?</h2>

<p><strong>Enceladus</strong> is a <strong>Dynamic Conformance Engine</strong> which allows data from different formats to be standardized to parquet and conformed to group-accepted common reference (e.g. data for country designation which are <strong>DE</strong> in one source system and <strong>Deutschland</strong> in another, can be conformed to <strong>Germany</strong>).</p>

<p>The project is comprised of three main components:</p>

<h3 id="anamewiemenasmenas"><a name="wie-menas"/>Menas</h3>

<p>This is the user-facing web client, used to <strong>specify the standardization schema</strong>, and <strong>define the steps required to conform</strong> a dataset. <br />
There are three models used to do this:</p>

<ul>
<li><strong>Dataset</strong>: Specifies where the dataset will be read from on HDFS (<strong>RAW</strong>), the conformance rules that will be applied to it, and where it will land on HDFS once it is conformed (<strong>PUBLISH</strong>)</li>

<li><strong>Schema</strong>: Specifies the schema towards which the dataset will be standardized</li>

<li><strong>Mapping Table</strong>: Specifies where tables with master reference data can be found (parquet on HDFS), which are used when applying Mapping conformance rules (e.g. the dataset uses <strong>Germany</strong>, which maps to the master reference <strong>DE</strong> in the mapping table)</li>
</ul>

<h3 id="anamewiestandardizationstandardization"><a name="wie-standardization"/>Standardization</h3>

<p>This is a Spark job which reads an input dataset in any of the supported formats and <strong>produces a parquet dataset with the Menas-specified schema</strong> as output. </p>

<h3 id="anamewieconformanceconformance"><a name="wie-conformance"/>Conformance</h3>

<p>This is a Spark job which <strong>applies the Menas-specified conformance rules to the standardized dataset</strong>.</p>

<h2 id="anamebuildhowtobuild"><a name="build"/>How to build</h2>

<h4 id="buildrequirements">Build requirements:</h4>

<ul>
<li><strong>Maven 3.5.4+</strong></li>

<li><strong>Java 8</strong></li>
</ul>

<p>Each module provides configuration file templates with reasonable default values.
Make a copy of the <code>*.properties.template</code> and <code>*.conf.template</code> files in each module's <code>src/resources</code> directory removing the <code>.template</code> extension. 
Ensure the properties there fit your environment.</p>

<h4 id="buildcommands">Build commands:</h4>

<ul>
<li>Without tests: <code>mvn clean package -DskipTests</code></li>

<li>With unit tests: <code>mvn clean package</code></li>

<li>With integration tests: <code>mvn clean package -Pintegration</code></li>

<li>With component preload file generated: <code>mvn clean package -PgenerateComponentPreload</code></li>
</ul>

<h2 id="anamerunhowtorun"><a name="run"/>How to run</h2>

<h4 id="menasrequirements">Menas requirements:</h4>

<ul>
<li><a href="https://tomcat.apache.org/download-90.cgi"><strong>Tomcat 8.5/9.0</strong> installation</a></li>

<li><a href="https://docs.mongodb.com/manual/administration/install-community/"><strong>MongoDB 4.0</strong> installation</a></li>

<li><a href="https://absaoss.github.io/spline/"><strong>Spline UI deployment</strong></a> - place the <a href="https://search.maven.org/remotecontent?filepath=za/co/absa/spline/spline-web/0.3.8/spline-web-0.3.8.war">spline.war</a>
in your Tomcat webapps directory (rename after downloading to <em>spline.war</em>); NB! don't forget to set up the <code>spline.mongodb.url</code> configuration for the <em>war</em></li>

<li><strong>HADOOP<em>CONF</em>DIR</strong> environment variable, pointing to the location of your hadoop configuration (pointing to a hadoop installation)</li>
</ul>

<p>The <em>Spline UI</em> can be omitted; in such case the <strong>Menas</strong> <code>za.co.absa.enceladus.spline.urlTemplate</code> setting should be set to empty string. </p>

<h4 id="deployingmenas">Deploying Menas</h4>

<p>Simply copy the <strong>menas.war</strong> file produced when building the project into Tomcat's webapps directory. </p>

<h4 id="speedupinitialloadingtimeofmenas">Speed up initial loading time of menas</h4>

<ul>
<li>Build the project with the generateComponentPreload profile. Component preload will greatly reduce the number of HTTP requests required for the initial load of Menas</li>

<li>Enable the HTTP compression</li>

<li>Configure <code>spring.resources.cache.cachecontrol.max-age</code> in <code>application.properties</code> of Menas for caching of static resources</li>
</ul>

<h4 id="standardizationandconformancerequirements">Standardization and Conformance requirements:</h4>

<ul>
<li><a href="https://spark.apache.org/downloads.html"><strong>Spark 2.4.3 (Scala 2.11)</strong> installation</a></li>

<li><a href="https://hadoop.apache.org/releases.html"><strong>Hadoop 2.7</strong> installation</a></li>

<li><strong>Menas</strong> running instance</li>

<li><strong>Menas Credentials File</strong> in your home directory or on HDFS (a configuration file for authenticating the Spark jobs with Menas) 


<ul>
<li><strong>Use with in-memory authentication</strong>
e.g. <code>~/menas-credential.properties</code>:</li></ul>
</li>
</ul>

<pre><code>username=user
password=changeme
</code></pre>

<ul>
<li><strong>Menas Keytab File</strong> in your home directory or on HDFS


<ul>
<li><strong>Use with kerberos authentication</strong>, see <a href="https://kb.iu.edu/d/aumh">link</a> for details on creating keytab files</li>

<li><strong>Directory structure</strong> for the <strong>RAW</strong> dataset should follow the convention of <code>&lt;path_to_dataset_in_menas&gt;/&lt;year&gt;/&lt;month&gt;/&lt;day&gt;/v&lt;dataset_version&gt;</code>. This date is specified with the <code>--report-date</code> option when running the <strong>Standardization</strong> and <strong>Conformance</strong> jobs.</li>

<li><strong>_INFO file</strong> must be present along with the <strong>RAW</strong> data on HDFS as per the above directory structure. This is a file tracking control measures via <a href="https://github.com/AbsaOSS/atum">Atum</a>, an example can be found <a href="examples/data/input/_INFO">here</a>.</li></ul>
</li>
</ul>

<h4 id="runningstandardization">Running Standardization</h4>

<pre><code>&lt;spark home&gt;/spark-submit \
--num-executors &lt;num&gt; \
--executor-memory &lt;num&gt;G \
--master yarn \
--deploy-mode &lt;client/cluster&gt; \
--driver-cores &lt;num&gt; \
--driver-memory &lt;num&gt;G \
--conf "spark.driver.extraJavaOptions=-Dmenas.rest.uri=&lt;menas_api_uri:port&gt; -Dstandardized.hdfs.path=&lt;path_for_standardized_output&gt;-{0}-{1}-{2}-{3} -Dspline.mongodb.url=&lt;mongo_url_for_spline&gt; -Dspline.mongodb.name=&lt;spline_database_name&gt; -Dhdp.version=&lt;hadoop_version&gt;" \
--class za.co.absa.enceladus.standardization.StandardizationJob \
&lt;standardization_&lt;build_version&gt;.jar&gt; \
--menas-auth-keytab &lt;path_to_keytab_file&gt; \
--dataset-name &lt;dataset_name&gt; \
--dataset-version &lt;dataset_version&gt; \
--report-date &lt;date&gt; \
--report-version &lt;data_run_version&gt; \
--raw-format &lt;data_format&gt; \
--row-tag &lt;tag&gt;
</code></pre>

<ul>
<li>Here <code>row-tag</code> is a specific option for <code>raw-format</code> of type <code>XML</code>. For more options for different types please see our WIKI.</li>

<li>In case Menas is configured for in-memory authentication (e.g. in dev environments), replace <code>--menas-auth-keytab</code> with <code>--menas-credentials-file</code></li>
</ul>

<h4 id="runningconformance">Running Conformance</h4>

<pre><code>&lt;spark home&gt;/spark-submit \
--num-executors &lt;num&gt; \
--executor-memory &lt;num&gt;G \
--master yarn \
--deploy-mode &lt;client/cluster&gt; \
--driver-cores &lt;num&gt; \
--driver-memory &lt;num&gt;G \
--conf 'spark.ui.port=29000' \
--conf "spark.driver.extraJavaOptions=-Dmenas.rest.uri=&lt;menas_api_uri:port&gt; -Dstandardized.hdfs.path=&lt;path_of_standardized_input&gt;-{0}-{1}-{2}-{3} -Dconformance.mappingtable.pattern=reportDate={0}-{1}-{2} -Dspline.mongodb.url=&lt;mongo_url_for_spline&gt; -Dspline.mongodb.name=&lt;spline_database_name&gt;" -Dhdp.version=&lt;hadoop_version&gt; \
--packages za.co.absa:enceladus-parent:&lt;version&gt;,za.co.absa:enceladus-conformance:&lt;version&gt; \
--class za.co.absa.enceladus.conformance.DynamicConformanceJob \
&lt;conformance_&lt;build_version&gt;.jar&gt; \
--menas-auth-keytab &lt;path_to_keytab_file&gt; \
--dataset-name &lt;dataset_name&gt; \
--dataset-version &lt;dataset_version&gt; \
--report-date &lt;date&gt; \
--report-version &lt;data_run_version&gt;
</code></pre>

<ul>
<li>In case Menas is configured for in-memory authentication (e.g. in dev environments), replace <code>--menas-auth-keytab</code> with <code>--menas-credentials-file</code></li>
</ul>

<h4 id="helperscriptsforrunningstandardizationandconformance">Helper scripts for running Standardization and Conformance</h4>

<p>The Scripts in <code>scripts</code> folder can be used to simplify command lines for running Standardization and Conformance jobs.</p>

<p>Steps to configure the scripts are as follows:</p>

<ul>
<li>Copy all the scripts in <code>scripts</code> directory to a location in your environment.</li>

<li>Copy <code>enceladus_env.template.sh</code> to <code>enceladus_env.sh</code>.</li>

<li>Change <code>enceladus_env.sh</code> according to your environment settings.</li>

<li>Use <code>run_standardization.sh</code> and <code>run_conformance.sh</code> scripts instead of directly invoking <code>spark-submit</code> to run your jobs.</li>
</ul>

<p>The syntax for running Standardization and Conformance is similar to running them using <code>spark-submit</code>. The only difference is that
you don't have to provide environment-specific settings. Several resource options, like driver memory and driver cores also have
default values and can be omitted. The number of executors is still a mandatory parameter.</p>

<p>The basic command to run Standardization becomes:</p>

<pre><code>&lt;path to scripts&gt;/run_standardization.sh \
--num-executors &lt;num&gt; \
--deploy-mode &lt;client/cluster&gt; \
--menas-auth-keytab &lt;path_to_keytab_file&gt; \
--dataset-name &lt;dataset_name&gt; \
--dataset-version &lt;dataset_version&gt; \
--report-date &lt;date&gt; \
--report-version &lt;data_run_version&gt; \
--raw-format &lt;data_format&gt; \
--row-tag &lt;tag&gt;
</code></pre>

<p>The basic command to run Conformance becomes:</p>

<pre><code>&lt;path to scripts&gt;/run_conformance.sh \
--num-executors &lt;num&gt; \
--deploy-mode &lt;client/cluster&gt; \
--menas-auth-keytab &lt;path_to_keytab_file&gt; \
--dataset-name &lt;dataset_name&gt; \
--dataset-version &lt;dataset_version&gt; \
--report-date &lt;date&gt; \
--report-version &lt;data_run_version&gt;
</code></pre>

<h2 id="anamecontributehowtocontribute"><a name="contribute"/>How to contribute</h2>

<p>Please see our <a href="CONTRIBUTING.md"><strong>Contribution Guidelines</strong></a>.</p>

<h2 id="anameusehowtouse"><a name="use"/>How to use</h2>

<p>In this section some more complex and less obvious usage patterns are going to be described.</p>

<h3 id="anameusestandardizationstandardization"><a name="use-standardization"/>Standardization</h3>

<h4 id="adjustingstandardizationofdata">Adjusting Standardization Of Data</h4>

<p><em>Standardization</em> can be influenced by <code>metadata</code> in the schema of the data. These are the possible properties taken
into account with the description of their purpose.</p>

<p>| Property | Target data type | Description | Example |
| --- | --- | --- | --- |
| <em>sourcecolumn</em> | any | The source column to provide data of the described column | <em>id</em> |
| <em>default</em> | any atomic type| Default value to use in case data are missing | <em>0</em> |
| <em>pattern</em> | date &amp; timestamp | Pattern for the date or timestamp representation | <em>dd.MM.yy</em> |
| <em>timezone</em> | timestamp (also date) | The time zone of the timestamp when that is not part of the pattern (NB! for date it can return unexpected results) | <em>US/Pacific</em> |</p>

<p>Schema entry example:
<code>
{
    "name": "MODIFIEDTIMESTAMP",
    "type": "timestamp",
    "nullable": true,
    "metadata": { <br />
        "description": "Timestamp when the row was last changed.",
        "sourcecolumn": "MODIFIED"
        "default": "1970/01/01 01-00-00"
        "pattern": "yyyy/MM/dd HH-mm-ss"
        "timezone": "CET"
    }
}
</code></p>

<h4 id="datetime">Date &amp; time</h4>

<p>Dates and especially timestamps (date + time) can be tricky. Currently Spark considers all time entries to be in the 
current system time zone by default. (For more detailed explanation of possible issues with that see 
<a href="https://docs.google.com/document/d/1gNRww9mZJcHvUDCXklzjFEQGpefsuR_akCDfWsdE35Q/edit#heading=h.n699ftkvhjlo">Consistent timestamp types in Hadoop SQL engines</a>.)</p>

<p>To address this potential source of discrepancies the following has been implemented:</p>

<ol>
<li>All Enceladus components are set to run in UTC</li>

<li>As part of <strong>Standardization</strong> all time related entries are normalized to UTC</li>

<li>There are several methods how to ensure that a timestamp entry is normalized as expected</li>

<li>We urge users, that all timestamp entries should include time zone information in one of the supported ways</li>

<li>While this is all valid for date entries too, it should be noted that UTC normalization of a date can have unexpected 
consequences - namely all dates west from UTC would be shifted to a day earlier</li>
</ol>

<h5 id="datetimestamppattern">Date &amp; timestamp pattern</h5>

<p>To enable processing of time entries from other systems <strong>Standardization</strong> offers the possibility to convert 
string and even numeric values to timestamp or date types. It's done using Spark's ability to convert strings to 
timestamp/date with some enhancements. The pattern placeholders and usage is described in Java's 
<a href="https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html"><code>SimpleDateFormat</code> class description</a> with 
the addition of recognizing two keywords <code>epoch</code> and <code>milliepoch</code> (case insensitive) to denote the number of 
seconds/milliseconds since epoch (1970/01/01 00:00:00.000 UTC).
It should be noted explicitly that <code>epoch</code> and <code>milliepoch</code> are considered a pattern including time zone.</p>

<p>Summary:</p>

<p>| placeholder | Description | Example |
| --- | --- | --- |
| G | Era designator | AD |
| y | Year | 1996; 96 |
| Y | Week year | 2009; 09 |
| M | Month in year (context sensitive) |  July; Jul; 07 |
| L | Month in year (standalone form) | July; Jul; 07 |
| w | Week in year | 27 |
| W | Week in month | 2 |
| D | Day in year | 189 |
| d | Day in month |  10 |
| F | Day of week in month | 2 |
| E | Day name in week | Tuesday; Tue |
| u | Day number of week (1 = Monday, ..., 7 = Sunday) | 1 |
| a | Am/pm marker | PM |
| H | Hour in day (0-23) | 0 |
| k | Hour in day (1-24) | 24 |
| K | Hour in am/pm (0-11) |  0 |
| h | Hour in am/pm (1-12) | 12 |
| m | Minute in hour | 30 |
| s | Second in minute | 55 |
| S | Millisecond | 978 |
| z | General time zone | Pacific Standard Time; PST; GMT-08:00 |
| Z | RFC 822 time zone | -0800 |
| X | ISO 8601 time zone | -08; -0800; -08:00 |
| <em>epoch</em> | Seconds since 1970/01/01 00:00:00 | 1557136493|
| <em>milliepoch</em> | Milliseconds since 1970/01/01 00:00:00.0000| 15571364938124 |</p>

<p><strong>NB!</strong> Spark uses US Locale and because on-the-fly conversion would be complicated, at the moment we stick to this 
hardcoded locale as well. E.g. <code>am/pm</code> for <code>a</code> placeholder, English names of days and months etc.</p>

<p><strong>NB!</strong> The keywords are case <strong>insensitive</strong>. Therefore, there is no difference between <code>epoch</code> and <code>EpoCH</code>.</p>

<h5 id="timezonesupport">Time Zone support</h5>

<p>As it has been mentioned, it's highly recommended to use timestamps with the time zone. But it's not unlikely that the 
source for standardization doesn't provide the time zone information. On the other hand, these times are usually within
one time zone. To ensure proper standardization, the schema's <em>metadata</em> can include the <code>timezone</code> value.
All timestamps then will be standardized as belonging to the particular time zone.  </p>

<p>E.g. <em>2019-05-04 11:31:10</em> with <code>timzene</code> specified as <em>CET</em> will be standardized to <em>2019-05-04 10:31:10</em> (UTC of 
course)</p>

<p>In case the pattern already includes information to recognize the time zone, the <code>timezone</code> entry in <em>metadata</em> will 
be ignored. Namely if the pattern includes 'z', 'Z' or 'X' placeholder or <code>epoch</code>/<code>milliepoch</code> keywords.</p>

<p><strong>NB!</strong> Due to spark limitation, only time zone ids are accepted as valid values. To get the full list of supported time
 zone denominators see the output of Java's 
<a href="https://docs.oracle.com/javase/8/docs/api/java/util/TimeZone.html#getAvailableIDs--"><code>TimeZone.getAvailableIDs()</code> function</a>. </p>

<h5 id="defaultvalue">Default value</h5>

<p>Default value is used to handle <strong>NULL</strong> values in non-nullable columns when they are being standardized. This can be due to type mismatch or <strong>NULL</strong> entries.
Date and timestamp default values, specifically, have to adhere to the provided <code>pattern</code>. If no pattern is 
provided, the implicit pattern is used - <code>yyyy-MM-dd</code> for dates and <code>yyyy-MM-dd HH:mm:ss</code> for timestamps.</p>
</div>